"""
Contract tests for backend-frontend data compatibility.

These tests ensure that data structures generated by the backend 
match the schemas expected by the frontend, preventing the type
of critical bug where frontend validation fails on backend data.
"""

import json
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, Any, List

import pytest

from lyrics_transcriber.types import (
    AnchorSequence, ScoredAnchor, GapSequence, Word, LyricsSegment, 
    WordCorrection, CorrectionResult, CorrectionStep, LyricsData,
    LyricsMetadata, PhraseScore, PhraseType
)
from tests.test_helpers import create_test_word, create_test_segment


class FrontendSchemaValidator:
    """Helper class to validate data against frontend schemas."""
    
    def __init__(self):
        # Path to the frontend validation script
        self.script_path = Path(__file__).parent.parent.parent / "lyrics_transcriber" / "frontend" / "scripts" / "validate-schemas.js"
        
    def validate_data(self, schema_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate data against a frontend schema.
        
        Args:
            schema_name: Name of the schema to validate against
            data: Data to validate
            
        Returns:
            Dictionary with validation results
            
        Raises:
            RuntimeError: If validation script fails to run
        """
        # Write data to temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(data, f, indent=2)
            temp_file = f.name
            
        try:
            # Run the validation script
            result = subprocess.run([
                'node', str(self.script_path),
                f'--schema={schema_name}',
                f'--file={temp_file}'
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                # Successful validation
                return json.loads(result.stdout)
            else:
                # Validation failed
                try:
                    error_data = json.loads(result.stdout)
                    return error_data
                except json.JSONDecodeError:
                    # If we can't parse the error, return a generic failure
                    return {
                        "success": False,
                        "valid": False,
                        "error": f"Validation failed: {result.stderr or result.stdout}"
                    }
                
        except subprocess.TimeoutExpired:
            raise RuntimeError("Frontend validation script timed out")
        except FileNotFoundError:
            raise RuntimeError("Node.js not found. Please install Node.js to run contract tests.")
        except Exception as e:
            raise RuntimeError(f"Failed to run frontend validation: {e}")
        finally:
            # Clean up temporary file
            try:
                os.unlink(temp_file)
            except OSError:
                pass


@pytest.fixture
def validator():
    """Provide a frontend schema validator."""
    return FrontendSchemaValidator()


def test_anchor_sequence_schema_compatibility(validator):
    """
    Test that AnchorSequence.to_dict() output matches frontend schema.
    
    This is the EXACT test that would have caught the original bug.
    """
    # Create anchor sequence using new API (not backwards compatibility mode)
    anchor = AnchorSequence(
        id="test-anchor-1",
        transcribed_word_ids=["word1", "word2", "word3"],
        transcription_position=0,
        reference_positions={"source1": 0, "source2": 5},
        reference_word_ids={
            "source1": ["ref1", "ref2", "ref3"],
            "source2": ["ref4", "ref5", "ref6"]
        },
        confidence=0.95
    )
    
    # Serialize as backend would
    serialized = anchor.to_dict()
    
    # Add required fields that would be added by ScoredAnchor
    scored_data = {
        **serialized,
        "phrase_score": {
            "phrase_type": "complete",
            "natural_break_score": 0.8,
            "length_score": 0.7,
            "total_score": 0.75
        },
        "total_score": 0.85
    }
    
    # Validate against frontend schema
    result = validator.validate_data("anchorSequence", scored_data)
    
    assert result["success"], f"Frontend validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "Data is not valid according to frontend schema"


def test_old_format_anchor_sequence_frontend_compatibility(validator):
    """
    Regression test: Verify old format cached data works with frontend after conversion.
    
    This test recreates the EXACT bug scenario:
    1. Old format cached data
    2. Backend loads and converts 
    3. Frontend validation should succeed
    """
    # Create anchor sequence data in OLD format (as it was cached)
    old_format_data = {
        "words": ["hello", "world", "test"],
        "text": "hello world test", 
        "length": 3,
        "transcription_position": 0,
        "reference_positions": {"source1": 0},
        "confidence": 1.0
    }
    
    # Simulate loading from cache (old format) and converting
    anchor = AnchorSequence.from_dict(old_format_data)
    serialized = anchor.to_dict()
    
    # Add required fields for complete schema validation
    scored_data = {
        **serialized,
        "phrase_score": {
            "phrase_type": "complete",
            "natural_break_score": 0.8,
            "length_score": 0.7,
            "total_score": 0.75
        },
        "total_score": 0.85
    }
    
    # Verify frontend can consume it
    result = validator.validate_data("anchorSequence", scored_data)
    
    assert result["success"], f"Old format data failed frontend validation: {result.get('errors', result.get('error'))}"
    assert result["valid"], "Converted old format data is not valid according to frontend schema"
    
    # Verify required fields are present
    assert "id" in serialized, "Missing 'id' field required by frontend"
    assert "transcribed_word_ids" in serialized, "Missing 'transcribed_word_ids' field required by frontend" 
    assert "reference_word_ids" in serialized, "Missing 'reference_word_ids' field required by frontend"
    
    # Verify old fields are NOT present (should be converted)
    assert "words" not in serialized, "Old 'words' field should not be in serialized output"


def test_gap_sequence_schema_compatibility(validator):
    """Test that GapSequence.to_dict() output matches frontend schema."""
    gap = GapSequence(
        id="gap-1",
        transcribed_word_ids=["word4", "word5"],
        transcription_position=3,
        preceding_anchor_id="anchor-1",
        following_anchor_id="anchor-2",
        reference_word_ids={"source1": ["ref7", "ref8"]}
    )
    
    serialized = gap.to_dict()
    result = validator.validate_data("gapSequence", serialized)
    
    assert result["success"], f"GapSequence validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "GapSequence data is not valid according to frontend schema"


def test_word_schema_compatibility(validator):
    """Test that Word.to_dict() output matches frontend schema."""
    word = create_test_word("hello", start_time=1.0, end_time=2.0, confidence=0.9)
    serialized = word.to_dict()
    
    result = validator.validate_data("word", serialized)
    
    assert result["success"], f"Word validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "Word data is not valid according to frontend schema"


def test_lyrics_segment_schema_compatibility(validator):
    """Test that LyricsSegment.to_dict() output matches frontend schema."""
    # Create a segment with words
    words = [
        create_test_word("hello", start_time=1.0, end_time=1.5),
        create_test_word("world", start_time=1.5, end_time=2.0)
    ]
    segment = create_test_segment("hello world", words, start_time=1.0, end_time=2.0)
    serialized = segment.to_dict()
    
    result = validator.validate_data("lyricsSegment", serialized)
    
    assert result["success"], f"LyricsSegment validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "LyricsSegment data is not valid according to frontend schema"


def test_word_correction_schema_compatibility(validator):
    """Test that WordCorrection.to_dict() output matches frontend schema."""
    correction = WordCorrection(
        id="correction-1",
        handler="test_handler",
        original_word="wrng",
        corrected_word="wrong",
        word_id="word1",
        corrected_word_id="word1-corrected",
        source="test",
        confidence=0.8,
        reason="spelling_correction",
        alternatives={"wrong": 5, "rung": 1},
        is_deletion=False,
        reference_positions={"source1": 0},
        length=1
    )
    
    serialized = correction.to_dict()
    result = validator.validate_data("wordCorrection", serialized)
    
    assert result["success"], f"WordCorrection validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "WordCorrection data is not valid according to frontend schema"


def test_correction_result_full_compatibility(validator):
    """
    Test complete CorrectionResult serialization compatibility.
    
    This is the most comprehensive test - it validates the entire
    data structure that gets sent to the frontend.
    """
    # Create test data
    word1 = create_test_word("hello", start_time=1.0, end_time=1.5)
    word2 = create_test_word("world", start_time=1.5, end_time=2.0)
    
    segment = create_test_segment("hello world", [word1, word2], start_time=1.0, end_time=2.0)
    
    anchor = AnchorSequence(
        id="anchor-1",
        transcribed_word_ids=[word1.id, word2.id],
        transcription_position=0,
        reference_positions={"source1": 0},
        reference_word_ids={"source1": ["ref1", "ref2"]},
        confidence=0.9
    )
    
    correction = WordCorrection(
        id="correction-1",
        handler="test_handler",
        original_word="hello",
        corrected_word="hello",
        word_id=word1.id,
        corrected_word_id=word1.id,
        source="test",
        confidence=0.9,
        reason="no_change",
        alternatives={},
        is_deletion=False,
        length=1
    )
    
    correction_step = CorrectionStep(
        handler_name="test_handler",
        affected_word_ids=[word1.id],
        affected_segment_ids=[segment.id],
        corrections=[correction],
        segments_before=[segment],
        segments_after=[segment]
    )
    
    # Create minimal LyricsData
    metadata = LyricsMetadata(
        source="test",
        track_name="Test Song",
        artist_names="Test Artist",
        lyrics_provider="test_provider",
        lyrics_provider_id="test_id"
    )
    
    lyrics_data = LyricsData(
        segments=[segment],
        metadata=metadata,
        source="test"
    )
    
    # Create CorrectionResult
    correction_result = CorrectionResult(
        original_segments=[segment],
        corrected_segments=[segment],
        corrections=[correction],
        corrections_made=1,
        confidence=0.9,
        reference_lyrics={"source1": lyrics_data},
        anchor_sequences=[anchor],
        gap_sequences=[],
        resized_segments=[segment],
        metadata={
            "anchor_sequences_count": 1,
            "gap_sequences_count": 0,
            "total_words": 2,
            "correction_ratio": 0.5,
        },
        correction_steps=[correction_step],
        word_id_map={word1.id: word1.id, word2.id: word2.id},
        segment_id_map={segment.id: segment.id}
    )
    
    # Serialize the complete result
    serialized = correction_result.to_dict()
    
    # Validate against frontend schema
    result = validator.validate_data("correctionData", serialized)
    
    assert result["success"], f"Complete CorrectionResult validation failed: {result.get('errors', result.get('error'))}"
    assert result["valid"], "Complete CorrectionResult data is not valid according to frontend schema"


def test_backend_frontend_contract_comprehensive():
    """
    Meta-test: Ensure all critical backend types have corresponding frontend tests.
    
    This test documents which backend-frontend contracts we're testing.
    """
    tested_types = [
        "AnchorSequence", 
        "GapSequence",
        "Word",
        "LyricsSegment", 
        "WordCorrection",
        "CorrectionResult"
    ]
    
    # This list should be updated when new types are added to the contract
    critical_backend_types = [
        "AnchorSequence",
        "GapSequence", 
        "Word",
        "LyricsSegment",
        "WordCorrection",
        "CorrectionResult",
        "LyricsData",
        "CorrectionStep"
    ]
    
    for type_name in critical_backend_types:
        assert any(type_name in test_name for test_name in tested_types + ["CorrectionResult"]), \
            f"Missing contract test for critical type: {type_name}"


if __name__ == "__main__":
    # Run contract tests
    pytest.main([__file__, "-v"]) 